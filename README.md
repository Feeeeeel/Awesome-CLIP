# Awesome CLIP 
This repo collects the research resources of CLIP (Contrastive Language-Image Pre-Training) proposed by OpenAI. If you would like to contribute, please open issues or pull requests. 

## CLIP 
- Learning Transferable Visual Models From Natural Language Supervision [[paper](https://arxiv.org/abs/2103.00020)][[code](https://github.com/openai/CLIP)]
- CLIP: Connecting Text and Images [[blog](https://openai.com/blog/clip/)]
- Multimodal Neurons in Artificial Neural Networks [[blog](https://openai.com/blog/multimodal-neurons/)]

## Training
- OpenCLIP (3rd-party, PyTorch) [[code](https://github.com/mlfoundations/open_clip)]  
- Train-CLIP (3rd-party, PyTorch) [[code](https://github.com/Zasder3/train-CLIP)] 
- Paddle-CLIP (3rd-party, PaddlePaddle) [[code](https://github.com/Zasder3/train-CLIP)] 


## Applications

### GAN 
- VQGAN-CLIP [[code](https://github.com/nerdyrodent/VQGAN-CLIP)]
- StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery [[paper](https://arxiv.org/abs/2103.17249)][[code](https://github.com/orpatashnik/StyleCLIP)]
- CLIP Guided Diffusion [[code](https://github.com/afiaka87/clip-guided-diffusion)] 
- CLIP2StyleGAN: Unsupervised Extraction of StyleGAN Edit Directions [[paper](https://arxiv.org/abs/2112.05219)]
- TargetCLIP: Image-Based CLIP-Guided Essence Transfer  [[paper](https://arxiv.org/abs/2110.12427)][[code](https://github.innominds.com/hila-chefer/TargetCLIP)]

### Object Detection
- Roboflow Zero-shot Object Tracking [[code](https://github.com/roboflow-ai/zero-shot-object-tracking)] 
- Zero-Shot Detection via Vision and Language Knowledge Distillation [[paper](https://arxiv.org/abs/2104.13921)][[code*](https://github.com/llrtt/Zero-Shot-Detection-via-Vision-and-Language-Knowledge-Distillation)]

### Information Retrieval
- Unsplash Image Search [[code](https://github.com/haltakov/natural-language-image-search)]
- CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval [[paper](https://arxiv.org/abs/2104.08860)][[code](https://github.com/ArrowLuo/CLIP4Clip)]
- Less is More: ClipBERT for Video-and-Language Learning via Sparse Sampling [[paper](https://arxiv.org/abs/2102.06183)][[code](https://github.com/jayleicn/ClipBERT)]
- Natural Language YouTube Search [[code](https://github.com/haltakov/natural-language-youtube-search)]

### Video Understanding
- VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding [[code]()]

### Image Captioning
- CLIP prefix captioning [[code](https://github.com/rmokady/CLIP_prefix_caption)]

### Image Editing 
- HairCLIP: Design Your Hair by Text and Reference Image [[code](https://github.com/wty-ustc/HairCLIP)]
- Crop-CLIP [[code](https://github.com/vijishmadhavan/Crop-CLIP)]
- CLIPstyler: Image Style Transfer with a Single Text Condition [[code](https://github.com/paper11667/CLIPstyler)]
- CLIPasso: Semantically-Aware Object Sketching [[paper](https://clipasso.github.io/clipasso/static/source/paper_CLIPasso_Semantically_Aware_Object_Sketching.pdf)][[code](https://clipasso.github.io/clipasso/)]

### Text-to-3D Generation
- CLIP-Forge: Towards Zero-Shot Text-to-Shape Generation, [[paper](https://arxiv.org/pdf/2110.02624.pdf)]

### Representation Learning
- Wav2CLIP: Learning Robust Audio Representations From CLIP [[code](https://github.com/descriptinc/lyrebird-Wav2CLIP)]
- CLIP-Lite: Information Efficient Visual Representation Learning from Textual Annotation [[paper](https://arxiv.org/abs/2112.07133)]
- RgionCLIP: Region-based Language-Image Pretraining [[Paper](https://arxiv.org/pdf/2112.09106.pdf)]
- CMA-CLIP: Cross-Modality Attention CLIP for Image-Text Classification [[paper](https://arxiv.org/abs/2112.03562)]

### Others
- Multilingual-CLIP [[code](https://github.com/FreddeFrallan/Multilingual-CLIP)]
- CLIP (With Haiku + Jax!) [[code](https://github.com/kingoflolz/CLIP_JAX)]
- CLIP-Event: Connecting Text and Images with Event Structures [[paper](https://arxiv.org/abs/2201.05078)][[code](https://github.com/limanling/clip-event)]
- How Much Can CLIP Benefit Vision-and-Language Tasks? [[Paper](https://openreview.net/forum?id=zf_Ll3HZWgy)]
- Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm (DeCLIP) [[Paper](https://arxiv.org/abs/2110.05208)][[code](https://github.com/Sense-GVT/DeCLIP)]


## Acknowledgment
Inspired by [Awesome Visual-Transformer](https://github.com/dk-liang/Awesome-Visual-Transformer)

